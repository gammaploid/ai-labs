{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03b5781f",
   "metadata": {},
   "source": [
    "# MLX ResNet Model Training\n",
    "\n",
    "This notebook demonstrates training a ResNet (Residual Network) model with approximately 2000 neurons using Apple's MLX framework on a random dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10e5349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27b21630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "mx.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb7a33bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating random dataset...\n",
      "Dataset shape: X=(10000, 64), y=(10000,)\n",
      "Features range: [0.000, 1.000]\n"
     ]
    }
   ],
   "source": [
    "# Generate random dataset\n",
    "def generate_random_dataset(n_samples: int = 10000, n_features: int = 64, n_classes: int = 10) -> Tuple[mx.array, mx.array]:\n",
    "    \"\"\"\n",
    "    Generate a random dataset for classification.\n",
    "    Using 64 features to simulate image-like data (8x8 \"images\")\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of samples\n",
    "        n_features: Number of input features (64 for 8x8 images)\n",
    "        n_classes: Number of output classes\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (features, labels)\n",
    "    \"\"\"\n",
    "    # Generate random features normalized to [0, 1] range like images\n",
    "    X = mx.random.uniform(0, 1, (n_samples, n_features))\n",
    "    \n",
    "    # Generate random labels\n",
    "    y = mx.random.randint(0, n_classes, (n_samples,))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate dataset\n",
    "print(\"Generating random dataset...\")\n",
    "X, y = generate_random_dataset(n_samples=10000, n_features=64, n_classes=10)\n",
    "print(f\"Dataset shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"Features range: [{X.min():.3f}, {X.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68910209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: X_train=(8000, 64), y_train=(8000,)\n",
      "Test set: X_test=(2000, 64), y_test=(2000,)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into train and test\n",
    "def train_test_split(X: mx.array, y: mx.array, test_size: float = 0.2) -> Tuple[mx.array, mx.array, mx.array, mx.array]:\n",
    "    \"\"\"\n",
    "    Split dataset into training and testing sets.\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    n_test = int(n_samples * test_size)\n",
    "    n_train = n_samples - n_test\n",
    "    \n",
    "    # Random indices for splitting\n",
    "    indices = mx.arange(n_samples)\n",
    "    train_indices = indices[:n_train]\n",
    "    test_indices = indices[n_train:]\n",
    "    \n",
    "    X_train = X[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "print(f\"Training set: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "print(f\"Test set: X_test={X_test.shape}, y_test={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a118277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet model initialized with 2000 neurons\n",
      "Layer sizes: [512, 512, 512, 464]\n",
      "Total parameters: 0\n"
     ]
    }
   ],
   "source": [
    "# Define ResNet Building Blocks\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Residual Block for ResNet.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int, stride: int = 1, downsample=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(in_features, out_features)\n",
    "        self.bn1 = nn.BatchNorm(out_features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(out_features, out_features)\n",
    "        self.bn2 = nn.BatchNorm(out_features)\n",
    "        self.downsample = downsample\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        # First linear layer\n",
    "        out = self.linear1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Second linear layer\n",
    "        out = self.linear2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # Skip connection\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out = out + identity  # Residual connection\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet architecture with approximately 2000 neurons.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, num_classes: int, num_neurons: int = 2000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Calculate layer sizes to achieve ~2000 neurons\n",
    "        # We'll use 4 residual blocks with decreasing sizes\n",
    "        layer1_size = 512  # 512 neurons\n",
    "        layer2_size = 512  # 512 neurons  \n",
    "        layer3_size = 512  # 512 neurons\n",
    "        layer4_size = 464  # 464 neurons (total ≈ 2000)\n",
    "        \n",
    "        # Initial projection layer\n",
    "        self.initial_linear = nn.Linear(input_size, layer1_size)\n",
    "        self.initial_bn = nn.BatchNorm(layer1_size)\n",
    "        self.initial_relu = nn.ReLU()\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.layer1 = self._make_layer(layer1_size, layer1_size, 2)\n",
    "        self.layer2 = self._make_layer(layer1_size, layer2_size, 2, \n",
    "                                      downsample=nn.Linear(layer1_size, layer2_size))\n",
    "        self.layer3 = self._make_layer(layer2_size, layer3_size, 2)\n",
    "        self.layer4 = self._make_layer(layer3_size, layer4_size, 2,\n",
    "                                      downsample=nn.Linear(layer3_size, layer4_size))\n",
    "        \n",
    "        # Global average pooling and classifier\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.classifier = nn.Linear(layer4_size, num_classes)\n",
    "        \n",
    "        # Store layer sizes for reporting\n",
    "        self.layer_sizes = [layer1_size, layer2_size, layer3_size, layer4_size]\n",
    "        self.total_neurons = sum(self.layer_sizes)\n",
    "    \n",
    "    def _make_layer(self, in_features: int, out_features: int, blocks: int, downsample=None):\n",
    "        \"\"\"\n",
    "        Create a layer with multiple residual blocks.\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "        \n",
    "        # First block (may have downsample)\n",
    "        layers.append(ResidualBlock(in_features, out_features, downsample=downsample))\n",
    "        \n",
    "        # Remaining blocks\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock(out_features, out_features))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Initial projection\n",
    "        x = self.initial_linear(x)\n",
    "        x = self.initial_bn(x)\n",
    "        x = self.initial_relu(x)\n",
    "        \n",
    "        # Residual layers\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        # Global pooling and classification\n",
    "        # Remove the mean operation since we don't need global pooling for 1D features\n",
    "        # x has shape (batch_size, layer4_size) already\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize the ResNet model\n",
    "model = ResNet(input_size=64, num_classes=10)\n",
    "print(f\"ResNet model initialized with {model.total_neurons} neurons\")\n",
    "print(f\"Layer sizes: {model.layer_sizes}\")\n",
    "\n",
    "# Count total parameters\n",
    "def count_parameters(model):\n",
    "    total = 0\n",
    "    for name, param in model.parameters().items():\n",
    "        if hasattr(param, 'size'):\n",
    "            total += param.size\n",
    "        elif hasattr(param, 'shape'):\n",
    "            total += np.prod(param.shape)\n",
    "    return total\n",
    "\n",
    "total_params = count_parameters(model)\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfd24fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer initialized: Adam with learning rate 0.001\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "def cross_entropy_loss(logits, targets):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss.\n",
    "    \"\"\"\n",
    "    return mx.mean(nn.losses.cross_entropy(logits, targets))\n",
    "\n",
    "def accuracy(logits, targets):\n",
    "    \"\"\"\n",
    "    Compute accuracy.\n",
    "    \"\"\"\n",
    "    predictions = mx.argmax(logits, axis=1)\n",
    "    return mx.mean(predictions == targets)\n",
    "\n",
    "# Initialize optimizer with learning rate scheduling\n",
    "initial_lr = 0.001\n",
    "optimizer = optim.Adam(learning_rate=initial_lr)\n",
    "print(f\"Optimizer initialized: Adam with learning rate {initial_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d0bcb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def forward_and_loss(model, X, y):\n",
    "    \"\"\"\n",
    "    Forward pass and loss computation.\n",
    "    \"\"\"\n",
    "    logits = model(X)\n",
    "    loss = cross_entropy_loss(logits, y)\n",
    "    return loss, logits\n",
    "\n",
    "# Compile the training step\n",
    "loss_and_grad_fn = nn.value_and_grad(model, forward_and_loss)\n",
    "\n",
    "def train_step(model, optimizer, X, y):\n",
    "    \"\"\"\n",
    "    Single training step.\n",
    "    \"\"\"\n",
    "    (loss, logits), grads = loss_and_grad_fn(model, X, y)\n",
    "    optimizer.update(model, grads)\n",
    "    return loss, logits\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, initial_lr):\n",
    "    \"\"\"\n",
    "    Adjust learning rate during training (cosine annealing).\n",
    "    \"\"\"\n",
    "    lr = initial_lr * 0.5 * (1 + math.cos(math.pi * epoch / 100))\n",
    "    optimizer.learning_rate = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdd3f67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ResNet Training...\n",
      "================================================================================\n",
      "Starting ResNet training for 100 epochs...\n",
      "Batch size: 128, Number of batches: 62\n",
      "Model has 2000 neurons and 0 parameters\n",
      "================================================================================\n",
      "Epoch   1/100 | Loss: 2.5821 | Train Acc: 0.0906 | Test Acc: 0.1010 | LR: 0.001000 | Time: 1.63s\n",
      "Epoch   1/100 | Loss: 2.5821 | Train Acc: 0.0906 | Test Acc: 0.1010 | LR: 0.001000 | Time: 1.63s\n",
      "Epoch  11/100 | Loss: 2.3316 | Train Acc: 0.0955 | Test Acc: 0.0985 | LR: 0.000976 | Time: 0.68s\n",
      "Epoch  11/100 | Loss: 2.3316 | Train Acc: 0.0955 | Test Acc: 0.0985 | LR: 0.000976 | Time: 0.68s\n",
      "Epoch  21/100 | Loss: 2.3110 | Train Acc: 0.1023 | Test Acc: 0.1015 | LR: 0.000905 | Time: 0.66s\n",
      "Epoch  21/100 | Loss: 2.3110 | Train Acc: 0.1023 | Test Acc: 0.1015 | LR: 0.000905 | Time: 0.66s\n",
      "Epoch  31/100 | Loss: 2.3090 | Train Acc: 0.0958 | Test Acc: 0.1000 | LR: 0.000794 | Time: 0.71s\n",
      "Epoch  31/100 | Loss: 2.3090 | Train Acc: 0.0958 | Test Acc: 0.1000 | LR: 0.000794 | Time: 0.71s\n",
      "Epoch  41/100 | Loss: 2.3052 | Train Acc: 0.1056 | Test Acc: 0.0980 | LR: 0.000655 | Time: 0.71s\n",
      "Epoch  41/100 | Loss: 2.3052 | Train Acc: 0.1056 | Test Acc: 0.0980 | LR: 0.000655 | Time: 0.71s\n",
      "Epoch  51/100 | Loss: 2.3051 | Train Acc: 0.1037 | Test Acc: 0.1070 | LR: 0.000500 | Time: 0.74s\n",
      "Epoch  51/100 | Loss: 2.3051 | Train Acc: 0.1037 | Test Acc: 0.1070 | LR: 0.000500 | Time: 0.74s\n",
      "Epoch  61/100 | Loss: 2.3035 | Train Acc: 0.1071 | Test Acc: 0.1075 | LR: 0.000345 | Time: 0.74s\n",
      "Epoch  61/100 | Loss: 2.3035 | Train Acc: 0.1071 | Test Acc: 0.1075 | LR: 0.000345 | Time: 0.74s\n",
      "Epoch  71/100 | Loss: 2.3032 | Train Acc: 0.1052 | Test Acc: 0.0935 | LR: 0.000206 | Time: 0.73s\n",
      "Epoch  71/100 | Loss: 2.3032 | Train Acc: 0.1052 | Test Acc: 0.0935 | LR: 0.000206 | Time: 0.73s\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[metal::malloc] Resource limit (499000) exceeded.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 68\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting ResNet Training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m train_losses, train_accs, test_accs, lrs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_resnet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\n\u001b[1;32m     71\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResNet training completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 35\u001b[0m, in \u001b[0;36mtrain_resnet\u001b[0;34m(model, optimizer, X_train, y_train, X_test, y_test, epochs, batch_size)\u001b[0m\n\u001b[1;32m     32\u001b[0m X_batch \u001b[38;5;241m=\u001b[39m X_train[start_idx:end_idx]\n\u001b[1;32m     33\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_train[start_idx:end_idx]\n\u001b[0;32m---> 35\u001b[0m loss, logits \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     38\u001b[0m epoch_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m accuracy(logits, y_batch)\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, optimizer, X, y)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03mSingle training step.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m (loss, logits), grads \u001b[38;5;241m=\u001b[39m loss_and_grad_fn(model, X, y)\n\u001b[0;32m---> 18\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, logits\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:29\u001b[0m, in \u001b[0;36mOptimizer.update\u001b[0;34m(self, model, gradients)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: Module, gradients: \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply the gradients to the parameters of the model and update the\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    model with the new parameters.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m                          via :func:`mlx.nn.value_and_grad`.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     model\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:109\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[0;34m(self, gradients, parameters)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Apply the update\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_single\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/mlx/utils.py:53\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(fn, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TreeType(\n\u001b[1;32m     48\u001b[0m         tree_map(fn, child, \u001b[38;5;241m*\u001b[39m(r[i] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest), is_leaf\u001b[38;5;241m=\u001b[39mis_leaf)\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, child \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tree)\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m---> 53\u001b[0m         k: \u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, child \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     55\u001b[0m     }\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(tree, \u001b[38;5;241m*\u001b[39mrest)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/mlx/utils.py:53\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(fn, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TreeType(\n\u001b[1;32m     48\u001b[0m         tree_map(fn, child, \u001b[38;5;241m*\u001b[39m(r[i] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest), is_leaf\u001b[38;5;241m=\u001b[39mis_leaf)\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, child \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tree)\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m---> 53\u001b[0m         k: \u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, child \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     55\u001b[0m     }\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(tree, \u001b[38;5;241m*\u001b[39mrest)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/mlx/utils.py:47\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(fn, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m     46\u001b[0m     TreeType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(tree)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTreeType\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     53\u001b[0m         k: tree_map(fn, child, \u001b[38;5;241m*\u001b[39m(r[k] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest), is_leaf\u001b[38;5;241m=\u001b[39mis_leaf)\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, child \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     55\u001b[0m     }\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/mlx/utils.py:48\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m     46\u001b[0m     TreeType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(tree)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TreeType(\n\u001b[0;32m---> 48\u001b[0m         \u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, child \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tree)\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     53\u001b[0m         k: tree_map(fn, child, \u001b[38;5;241m*\u001b[39m(r[k] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest), is_leaf\u001b[38;5;241m=\u001b[39mis_leaf)\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, child \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     55\u001b[0m     }\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/mlx/utils.py:53\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(fn, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TreeType(\n\u001b[1;32m     48\u001b[0m         tree_map(fn, child, \u001b[38;5;241m*\u001b[39m(r[i] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest), is_leaf\u001b[38;5;241m=\u001b[39mis_leaf)\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, child \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tree)\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m---> 53\u001b[0m         k: \u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, child \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     55\u001b[0m     }\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(tree, \u001b[38;5;241m*\u001b[39mrest)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/mlx/utils.py:53\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(fn, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TreeType(\n\u001b[1;32m     48\u001b[0m         tree_map(fn, child, \u001b[38;5;241m*\u001b[39m(r[i] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest), is_leaf\u001b[38;5;241m=\u001b[39mis_leaf)\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, child \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tree)\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m---> 53\u001b[0m         k: \u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, child \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     55\u001b[0m     }\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(tree, \u001b[38;5;241m*\u001b[39mrest)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/mlx/utils.py:57\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(fn, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     53\u001b[0m         k: tree_map(fn, child, \u001b[38;5;241m*\u001b[39m(r[k] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest), is_leaf\u001b[38;5;241m=\u001b[39mis_leaf)\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, child \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     55\u001b[0m     }\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:523\u001b[0m, in \u001b[0;36mAdam.apply_single\u001b[0;34m(self, gradient, parameter, state)\u001b[0m\n\u001b[1;32m    521\u001b[0m m \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    522\u001b[0m v \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 523\u001b[0m m \u001b[38;5;241m=\u001b[39m b1 \u001b[38;5;241m*\u001b[39m m \u001b[38;5;241m+\u001b[39m \u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\n\u001b[1;32m    524\u001b[0m v \u001b[38;5;241m=\u001b[39m b2 \u001b[38;5;241m*\u001b[39m v \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m b2) \u001b[38;5;241m*\u001b[39m mx\u001b[38;5;241m.\u001b[39msquare(gradient)\n\u001b[1;32m    525\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [metal::malloc] Resource limit (499000) exceeded."
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "def train_resnet(model, optimizer, X_train, y_train, X_test, y_test, epochs=100, batch_size=128):\n",
    "    \"\"\"\n",
    "    Train the ResNet model.\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    learning_rates = []\n",
    "    \n",
    "    n_batches = len(X_train) // batch_size\n",
    "    \n",
    "    print(f\"Starting ResNet training for {epochs} epochs...\")\n",
    "    print(f\"Batch size: {batch_size}, Number of batches: {n_batches}\")\n",
    "    print(f\"Model has {model.total_neurons} neurons and {total_params:,} parameters\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_acc = 0.0\n",
    "        \n",
    "        # Adjust learning rate\n",
    "        current_lr = adjust_learning_rate(optimizer, epoch, initial_lr)\n",
    "        learning_rates.append(current_lr)\n",
    "        \n",
    "        # Training\n",
    "        for batch_idx in range(n_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            \n",
    "            X_batch = X_train[start_idx:end_idx]\n",
    "            y_batch = y_train[start_idx:end_idx]\n",
    "            \n",
    "            loss, logits = train_step(model, optimizer, X_batch, y_batch)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += accuracy(logits, y_batch).item()\n",
    "        \n",
    "        # Average metrics\n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        avg_train_acc = epoch_acc / n_batches\n",
    "        \n",
    "        # Test accuracy\n",
    "        test_logits = model(X_test)\n",
    "        test_acc = accuracy(test_logits, y_test).item()\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(avg_loss)\n",
    "        train_accuracies.append(avg_train_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch+1:3d}/{epochs} | \"\n",
    "                  f\"Loss: {avg_loss:.4f} | \"\n",
    "                  f\"Train Acc: {avg_train_acc:.4f} | \"\n",
    "                  f\"Test Acc: {test_acc:.4f} | \"\n",
    "                  f\"LR: {current_lr:.6f} | \"\n",
    "                  f\"Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    return train_losses, train_accuracies, test_accuracies, learning_rates\n",
    "\n",
    "# Start training\n",
    "print(\"Starting ResNet Training...\")\n",
    "print(\"=\" * 80)\n",
    "train_losses, train_accs, test_accs, lrs = train_resnet(\n",
    "    model, optimizer, X_train, y_train, X_test, y_test, \n",
    "    epochs=100, batch_size=128\n",
    ")\n",
    "print(\"=\" * 80)\n",
    "print(\"ResNet training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c515e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comprehensive training results\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "plt.title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(train_accs, 'g-', label='Training Accuracy', linewidth=2)\n",
    "plt.title('Training Accuracy Over Time', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot both training and test accuracy\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.plot(train_accs, 'g-', label='Training Accuracy', linewidth=2)\n",
    "plt.plot(test_accs, 'r-', label='Test Accuracy', linewidth=2)\n",
    "plt.title('Training vs Test Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot learning rate schedule\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.plot(lrs, 'm-', label='Learning Rate', linewidth=2)\n",
    "plt.title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Plot loss (log scale)\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.semilogy(train_losses, 'b-', label='Training Loss (Log Scale)', linewidth=2)\n",
    "plt.title('Training Loss (Log Scale)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross-Entropy Loss (Log)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy difference\n",
    "plt.subplot(2, 3, 6)\n",
    "acc_diff = [train - test for train, test in zip(train_accs, test_accs)]\n",
    "plt.plot(acc_diff, 'orange', label='Train-Test Accuracy Gap', linewidth=2)\n",
    "plt.title('Overfitting Monitor', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Difference')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final results\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"=\"*50)\n",
    "print(f\"Final Training Accuracy: {train_accs[-1]:.4f} ({train_accs[-1]*100:.2f}%)\")\n",
    "print(f\"Final Test Accuracy: {test_accs[-1]:.4f} ({test_accs[-1]*100:.2f}%)\")\n",
    "print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Best Test Accuracy: {max(test_accs):.4f} ({max(test_accs)*100:.2f}%) at epoch {test_accs.index(max(test_accs))+1}\")\n",
    "print(f\"Final Learning Rate: {lrs[-1]:.6f}\")\n",
    "print(f\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9a9e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet Model Architecture Summary\n",
    "print(\"ResNet Model Architecture Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input Layer: 64 features\")\n",
    "print(f\"Initial Projection: 64 → {model.layer_sizes[0]} neurons\")\n",
    "print(\"\\nResidual Blocks:\")\n",
    "for i, size in enumerate(model.layer_sizes, 1):\n",
    "    print(f\"  Layer {i}: {size} neurons (2 ResBlocks with skip connections)\")\n",
    "print(f\"\\nOutput Layer: {model.layer_sizes[-1]} → 10 classes\")\n",
    "print(f\"\\nTotal Hidden Neurons: {model.total_neurons}\")\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Architecture: ResNet with Batch Normalization and Dropout\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Analyze residual connections\n",
    "print(\"\\nResNet Features:\")\n",
    "print(\"• Skip connections for gradient flow\")\n",
    "print(\"• Batch normalization for training stability\")\n",
    "print(\"• Dropout for regularization\")\n",
    "print(\"• Cosine annealing learning rate schedule\")\n",
    "print(\"• Global average pooling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae6137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions and model analysis\n",
    "print(\"\\nModel Performance Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Test predictions on a sample\n",
    "sample_X = X_test[:10]\n",
    "sample_y = y_test[:10]\n",
    "sample_logits = model(sample_X)\n",
    "sample_preds = mx.argmax(sample_logits, axis=1)\n",
    "sample_probs = nn.softmax(sample_logits, axis=1)\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "for i in range(10):\n",
    "    true_label = sample_y[i].item()\n",
    "    pred_label = sample_preds[i].item()\n",
    "    confidence = sample_probs[i, pred_label].item()\n",
    "    status = \"✓\" if true_label == pred_label else \"✗\"\n",
    "    print(f\"Sample {i+1:2d}: True={true_label}, Pred={pred_label}, Conf={confidence:.3f} {status}\")\n",
    "\n",
    "# Calculate confusion matrix elements\n",
    "all_preds = mx.argmax(model(X_test), axis=1)\n",
    "correct_per_class = {}\n",
    "total_per_class = {}\n",
    "\n",
    "for i in range(10):\n",
    "    mask = y_test == i\n",
    "    if mx.sum(mask) > 0:\n",
    "        class_preds = all_preds[mask]\n",
    "        correct_per_class[i] = mx.sum(class_preds == i).item()\n",
    "        total_per_class[i] = mx.sum(mask).item()\n",
    "\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "for i in range(10):\n",
    "    if i in total_per_class and total_per_class[i] > 0:\n",
    "        acc = correct_per_class[i] / total_per_class[i]\n",
    "        print(f\"Class {i}: {acc:.3f} ({correct_per_class[i]}/{total_per_class[i]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efadaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ResNet vs Standard NN\n",
    "print(\"\\nResNet vs Standard Neural Network:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"ResNet Advantages:\")\n",
    "print(\"• Skip connections prevent vanishing gradients\")\n",
    "print(\"• Easier to train deeper networks\")\n",
    "print(\"• Better gradient flow through residual paths\")\n",
    "print(\"• Batch normalization improves training stability\")\n",
    "print(\"• Can achieve better performance with same parameter count\")\n",
    "print(\"\\nArchitectural Benefits:\")\n",
    "print(f\"• {len(model.layer_sizes)} residual layers with skip connections\")\n",
    "print(f\"• Total of {model.total_neurons} neurons across hidden layers\")\n",
    "print(f\"• Batch normalization in every residual block\")\n",
    "print(f\"• Dropout regularization (0.1 in blocks, 0.5 before classifier)\")\n",
    "print(f\"• Cosine annealing learning rate schedule\")\n",
    "\n",
    "# Save model information\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ResNet Training Completed Successfully!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nTo save this ResNet model:\")\n",
    "print(\"```python\")\n",
    "print(\"# Save model parameters\")\n",
    "print(\"mx.save_safetensors('resnet_model.safetensors', model.parameters())\")\n",
    "print(\"```\")\n",
    "print(\"\\nTo load the model later:\")\n",
    "print(\"```python\")\n",
    "print(\"# Load model\")\n",
    "print(\"model = ResNet(input_size=64, num_classes=10)\")\n",
    "print(\"model.load_weights(mx.load('resnet_model.safetensors'))\")\n",
    "print(\"```\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
